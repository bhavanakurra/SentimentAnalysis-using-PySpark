{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DB_Primarymodel_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyFnZ-B_6oQc",
        "outputId": "db28e2fc-f577-449e-e9fb-ea58444d5eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up spark"
      ],
      "metadata": {
        "id": "ZoYB4U_-_GY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "-KfktMeE-e5s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "E2-aWWgS-pcd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install file"
      ],
      "metadata": {
        "id": "EU9_gzB--rDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0a00db-aef8-4216-819c-22eb4b2c4308"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc libmagic1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 275 kB of archives.\n",
            "After this operation, 5,297 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Fetched 275 kB in 1s (493 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 155565 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mAYn1JwNGnJ",
        "outputId": "4f0cd083-bf17-4057-8f22-3142e58e48bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  spark-3.2.0-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!file spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blhse4p9-uPW",
        "outputId": "896100f0-94fd-4dfa-cded-26c6757b719e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.2.0-bin-hadoop3.2.tgz: gzip compressed data, from Unix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "hgLdTZwY-7_c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "SPAbulI3-_Ww"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "2N0s_p-Z_M1H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "wUm0nY_N_QXb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xCWSzBEK_S8A",
        "outputId": "979a94ad-c99b-4b31-afd0-5cded1a8cd08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.2.0-bin-hadoop3.2'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code"
      ],
      "metadata": {
        "id": "agB90j6XMzuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[4]\").appName(\"Colab\").config('spark.ui.port', '4050').getOrCreate()"
      ],
      "metadata": {
        "id": "L-CYEuWl_VVE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "sc.getConf().getAll()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxFpTDLDNQnx",
        "outputId": "90e87ebf-41bf-4667-8154-a9ea2538f7b4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.app.name', 'Colab'),\n",
              " ('spark.driver.host', '6db2e71ff6fe'),\n",
              " ('spark.master', 'local[4]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              " ('spark.driver.port', '41307'),\n",
              " ('spark.ui.port', '4050'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.app.id', 'local-1639188694967'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.startTime', '1639188693150')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "zCSJSSCL_Y8t",
        "outputId": "44ab503e-8821-490d-8324-663f34038c2c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6db2e71ff6fe:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[4]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4b5848d650>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, IntegerType, StringType\n",
        "schema = StructType().add(\"basemodel_review\",StringType(),True).add(\"review\",StringType(),True).add(\"result\",IntegerType(),True)\n",
        "df = spark.read.schema(schema).csv(\"/content/drive/My Drive/DB/Review/reviews_processed_final/review1.csv\", sep=\",\", multiLine=True, header=\"true\")\n",
        "df = df.select(\"review\",\"result\")\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI6lf3ghAPjc",
        "outputId": "1ad700a7-faad-4d5a-ea86-9f0f1b0b3e20"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- review: string (nullable = true)\n",
            " |-- result: integer (nullable = true)\n",
            "\n",
            "+--------------------+------+\n",
            "|              review|result|\n",
            "+--------------------+------+\n",
            "|\"\"\"apparently pri...|     1|\n",
            "|\"\"\"store pretty g...|     1|\n",
            "|\"\"\"call wvm recom...|     1|\n",
            "|\"\"\"ive stay many ...|     0|\n",
            "|\"\"\"food alway gre...|     1|\n",
            "+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "qy0Qx3qVAkrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"result\").distinct().show()"
      ],
      "metadata": {
        "id": "towQoDCTAqjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select(\"review\",\"result\")\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "ley4yTw1AzTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Balancing Data\n",
        "positive_reviews = df.filter(df.result == 1).limit(335748)\n",
        "negative_reviews = df.filter(df.result==0).limit(335748)"
      ],
      "metadata": {
        "id": "12o_LXa8BtUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(positive_reviews.count())\n",
        "print(negative_reviews.count())"
      ],
      "metadata": {
        "id": "lhyXoyGICzHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_main = positive_reviews.union(negative_reviews)\n",
        "df_main.printSchema()\n",
        "df_main.show(5)"
      ],
      "metadata": {
        "id": "P-fCfQJLFTFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_main.count()"
      ],
      "metadata": {
        "id": "UIHG3Za2FlBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_set, val_set, test_set) = df.randomSplit([0.98, 0.00, 0.02], seed = 2000)"
      ],
      "metadata": {
        "id": "cfzZ0jb6B9dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")  \n",
        "label_stringIdx = StringIndexer(inputCol = \"result\", outputCol = \"label\")\n",
        "pipeline = Pipeline(stages=[tokenizer, cv, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "train_df = pipelineFit.transform(train_set)\n",
        "test_df = pipelineFit.transform(test_set)\n",
        "train_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1MLkE-bDyBf",
        "outputId": "8d38fbaa-5aeb-49af-c288-7a0cb4686b62"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+------+--------------------+-----+\n",
            "|review|result| words|            features|label|\n",
            "+------+------+------+--------------------+-----+\n",
            "|  \"\"\"\"|     0|[\"\"\"\"]|(262144,[44772],[...|  1.0|\n",
            "|  \"\"\"\"|     0|[\"\"\"\"]|(262144,[44772],[...|  1.0|\n",
            "|  \"\"\"\"|     0|[\"\"\"\"]|(262144,[44772],[...|  1.0|\n",
            "|  \"\"\"\"|     0|[\"\"\"\"]|(262144,[44772],[...|  1.0|\n",
            "|  \"\"\"\"|     0|[\"\"\"\"]|(262144,[44772],[...|  1.0|\n",
            "+------+------+------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.select('features','label')\n",
        "test_df = test_df.select('features','label')\n",
        "train_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG_i4n9RQrUI",
        "outputId": "9cdcb820-0208-420a-c54a-4fa95096bbab"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(262144,[44772],[...|  1.0|\n",
            "|(262144,[44772],[...|  1.0|\n",
            "|(262144,[44772],[...|  1.0|\n",
            "|(262144,[44772],[...|  1.0|\n",
            "|(262144,[44772],[...|  1.0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "lr = LogisticRegression(maxIter=10)\n",
        "lrModel = lr.fit(train_df)\n",
        "end = time.time()"
      ],
      "metadata": {
        "id": "IhViXCdLOIx4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "train_predictions = lrModel.transform(train_df)\n",
        "\n",
        "results1 = train_predictions.select(['prediction', 'label'])\n",
        "predictionAndLabels1=results1.rdd\n",
        "metrics1 = MulticlassMetrics(predictionAndLabels1)\n",
        "cm1=metrics1.confusionMatrix().toArray()\n",
        "\n",
        "\n",
        "train_accuracy=(cm1[0][0]+cm1[1][1])/cm1.sum()\n",
        "print('train_accuracy : ', train_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSmcm7hfmRhy",
        "outputId": "a582b2d5-6e35-4918-8731-aecad300602c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_accuracy :  0.9469394084383184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lrModel.transform(test_df) \n",
        "test_accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_df.count())\n",
        "print('test_accuracy : ',test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "374HxgWBR0Ry",
        "outputId": "79f91bd5-586e-44a4-d2a9-13b3656369b0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy :  0.8913098427784237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "results = predictions.select(['prediction', 'label'])\n",
        "predictionAndLabels=results.rdd\n",
        "metrics = MulticlassMetrics(predictionAndLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBY_qKf4qkBr",
        "outputId": "0fa56c89-609b-4c9f-8d58-4dd9296dc10a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm=metrics.confusionMatrix().toArray()\n",
        "accuracy=(cm[0][0]+cm[1][1])/cm.sum()\n",
        "precision=(cm[0][0])/(cm[0][0]+cm[1][0])\n",
        "recall=(cm[0][0])/(cm[0][0]+cm[0][1])\n",
        "\n",
        "F1_score = (2*(precision*recall))/(precision+recall)"
      ],
      "metadata": {
        "id": "ebVwCv-Fquwo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('F1_score : ',F1_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD1qW4furaPG",
        "outputId": "e925d0a3-7013-4fc3-fc05-bf685ef86c38"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1_score :  0.9179764993132916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ma_v4bcTrfxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}